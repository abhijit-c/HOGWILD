\section{Introduction}

Despite being the subject of much modern excitement, the ideas of gradient
descent date all the way back to Cauchy in 1847. And although it's main
application has been in the solution of recent big-data optimization problems,
stochastic gradient has been around since the 1940s, formally by Robbins and
Monro in 1951. In the last two decades, however, modern hardware has begun to
see a tapering off of Moore's law, and has begun to expand out in a distributed
fashion with multicore processors and GPUs; naturally the question becomes: In
order to take advantage of the strengths of modern hardware, how can we
parallelize a stochastic gradient method?
