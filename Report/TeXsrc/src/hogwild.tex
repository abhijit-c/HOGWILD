\section{\hogwild}

Prior to 2011, parallel stochastic gradient methods had been introduced, but
most suffered from poor scaling due to the necessity of locks. A naive
implementation could look like:
\begin{breakablealgorithm}
  \caption{Very Naive Parallel Stochastic Gradient}
  \label{alg:naivePSG}
  \begin{algorithmic}[1]
    \Require Number of data points $N$, seperable loss function $f
    = \sum_{e \in E} f_e(x_e)$, Initial $x$.
    \For{epoch $= 1 \to$ MAX\_EPOCHS}
      \State \#pragma omp parallel for
      \For{$k = 1 \to N$}
        \State Choose $i$ uniformly from $\{1, \dots, |E|\}$.
        \State \#pragma omp critical
        \Indent
          \State Read current parameters $x$.
          \State Compute $\nabla f_i(x)$.
          \State $x \gets x - \eta \nabla f_i(x)$.
        \EndIndent
      \EndFor
    \EndFor
  \end{algorithmic}
\end{breakablealgorithm}
Note that the version presented above is one with a fixed number of iterations,
as the discussion of stopping criteria seems to be similar to that of the
stochastic gradient method, and for extremely large data sets, is often
heuristic. But it's clear here that such an algorithm would only effectively be
parallelizing the unform sample of $i$ in $\{1, \dots, |E|\}$, and it's overall
parallel efficiency would likely be poor. Technically, you can improve the above
by replacing the critical section with selective locks on components of $x$
based on the sparsity pattern of $\nabla f_i(x)$, but because the process of
acquiring locks is much more expensive than floating point arithmetic, this
helps little.

However, in 2011, the article "\hogwild: A Lock-Free Approach to Parallelizing
Stochastic Gradient Descent" by Niu et al. \cite{2011NRRW} proposed a very
simple solution to this problem. Remove the locks!%
\footnote{
  Apparently this was discovered by accident by Feng Niu, one of the original
  paper's authors, when he was debugging stochastic gradient method code. I wish
  my troubleshooting was nearly as effective... \cite{2014Recht}
}
\begin{breakablealgorithm}
  \caption{\hogwild: Asynchronous Stochastic Gradient with replacement}
  \label{alg:hogwildwreplacement}
  \begin{algorithmic}[1]
    \Require Number of data points $N$, seperable loss function $f
    = \sum_{e \in E} f_e(x_e)$, Initial $x$.
    \For{epoch $= 1 \to$ MAX\_EPOCHS}
      \State \#pragma omp parallel for
      \For{$k = 1 \to N$}
        \State Choose $i$ uniformly from $\{1, \dots, |E|\}$.
        \State Read current parameters $x$.
        \State Compute $\nabla f_i(x)$.
        \State $x \gets x - \eta \nabla f_i(x)$. \Comment{Must be done
        atomically}
      \EndFor
    \EndFor
  \end{algorithmic}
\end{breakablealgorithm}
and should we want to sample without replacement the algorithm is easily
modified to:
\begin{breakablealgorithm}
  \caption{\hogwild: Asynchronous Stochastic Gradient without replacement}
  \label{alg:hogwildworeplacement}
  \begin{algorithmic}[1]
    \Require Number of data points $N$, seperable loss function $f
    = \sum_{e \in E} f_e(x_e)$, Initial $x$.
    \For{epoch $= 1 \to$ MAX\_EPOCHS}
      \State Let $P$ be a random permutation of $\{1, \dots, |E|\}$.
      \Comment{i.e. a Fisher-Yates Shuffle.}
      \State \#pragma omp parallel for
      \For{$k = 1 \to N$}
        \State $i \gets P[k]$.
        \State Read current parameters $x$.
        \State Compute $\nabla f_i(x)$.
        \State $x \gets x - \eta \nabla f_i(x)$. \Comment{Must be done
        atomically}
      \EndFor
    \EndFor
  \end{algorithmic}
\end{breakablealgorithm}

It should be noted that although the formal OMP locks have been removed, atomic
operations are still required in order to prevent mutual exclusion. However, no
guards have been placed to prevent a thread from overwriting another's
computation midway through, and it's not obvious as to why such a race condition
wouldn't destroy the performance of the Stochastic Gradient method. However,
with certain assumptions one can show that \hogwild\ behaves roughly like a noisy
stochastic gradient method, and thus shares its convergence properties.
