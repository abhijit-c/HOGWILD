\section{Convergence Analysis}

Given that the result of \hogwild, in the absence of noise generated by the
asynchronous updates of $x$, henceforth denoted {\it asynchronous noise}, is
equivalent to a stochastic gradient method, we should expect convergence rates
similar to those of stochastic gradient, should noise be small. Take for example
the typical linear least squares loss function $f(x) = \frac{1}{2n}
\norm{Dx-b}_2^2$, where $x \in \mathbb{R}^n$ and $D \in \mathbb{R}^{n \times n}$
a diagonal matrix. Writing this as a sum of each data entry:
\[
  f(x) 
  = \frac{1}{2n} \sum_{k=1}^n (D_{ii}x_i - b_i)^2
  = \frac{1}{n} \sum_{k=1}^n f_i(x)
  \implies
  (\nabla f_i(x))_k
  =
  \begin{cases}
    D_{ii}(D_{ii}x_i - b_i), & k = i \\
    0, & k = 0
  \end{cases}
\]
Because our $\nabla f_i(x)$'s only have a single entry in their own component, we
can see that as long as no other thread is working on component $i$
simultaneously, no asynchronous noise will be generated; should we use \hogwild\
without replacement then this is guaranteed. In the original paper
\cite{2011NRRW}, sparsity of the vector $\nabla f_i(x)$ was required in order to
guarantee convergence, but as we'll see later, this isn't always necessary.

As a baseline of comparison, we first state a result on the convergence of the
stochastic gradient method:
\begin{theorem} \label{thm:sgd}
  (Convergence of the Stochastic Gradient Method \cite{2016BCN}) Let $F:
  \mathbb{R}^d \to \mathbb{R}$ be an objective function we're seeking to
  minimize. We can write this as either an expected risk $F(x)
  = \mathbb{E}_\xi{f(x, \xi)}$, or an empirical risk $F(x) = \frac{1}{n}
  \sum_{k=1}^n f_k(x)$.  Under the assumptions:
  \begin{enumerate}[(1)]
    \item $F$ is continuously differentiable and $\nabla F$ is Lipschitz
      continuous with Lipshitz constant $L$, i.e.
      \[
        \norm{\nabla F(x) - \nabla F(y)} 
        \leq
        L\norm{x-y}, \forall x,y \in \mathbb{R}^d
      \]
    \item $F$ is strongly convex with constant $c$, i.e.
      \[
        F(x) \geq F(y) + \nabla F(y)^T (x-y) + \frac{1}{2}c\norm{x-y}^2, 
        \forall x,y \in \mathbb{R}^d
      \]
    \item $F$ is bounded below over the region explored by stochastic gradient
      method.
    \item In expectation, the vector $-\nabla f(x_k, \xi_k)$ is a descent
      direction for $F$ with norm bounded by it's own norm. That is: $\exists
      \mu_G \geq \mu > 0$ such that $\forall k \in \mathbb{N}$:
      \[
        \nabla F(x_k)^T \E{\nabla f(x_k,\xi_k)} \geq \mu \norm{\nabla F(x_k)}^2
        \text{ and }
        \norm{ \E{\nabla f(x_k,\xi_k)} } \geq \mu_G \norm{\nabla F(x_k)}
      \]
    \item $\exists M, M_V \geq 0$ such that $\forall k\in \mathbb{N}$:
      \[
        \Var{\nabla f(x_k, \xi_k)} \leq M + M_V \norm{\nabla F(x_k)}^2
      \]
  \end{enumerate}
  Then, assuming a fixed stepsize $\alpha$ (a.k.a. learning rate), satisfying
  $\alpha \in (0, \mu/ LM_g]$, we have:
  \[
    \E{F(x_k) - F_*} 
    \leq 
    \frac{\alpha LM}{2c\mu} + (1 - \alpha c \mu)^{k-1}
    \left(
      F(x_1) - F_* - \frac{\alpha LM}{2c\mu}
    \right)
  \]
  and if we instead choose $\alpha$ diminishing, i.e. let $\alpha_k
  = \frac{\beta}{\gamma + k}$ where $\beta > 1/c\mu, \gamma > 0$ are chose such
  that $\alpha_1 \leq \mu/LM_G$, then:
  \[
    \E{F(x_k) - F_*} 
    \leq 
    \frac{1}{\gamma + k} \max
    \left\{
      \frac{\beta^2 LM}{2(\beta c \mu -1)},
      (\gamma + 1)(F(x) - F_*)
    \right\}
  \]
\end{theorem}
The result is technical, and very long to prove, so I just refer to the article
\cite{2016BCN} for it. Regardless, the last result in the above theorem is what
we strive for in $\hogwild$: convergence in $\mathcal{O}(1/k)$ time.

\input{./src/convergence/theory.tex}
\input{./src/convergence/numerical.tex}
